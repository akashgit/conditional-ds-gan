{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import sys\n",
    "import gc\n",
    "sys.path.append('../wtvae/src/')\n",
    "from vae_models import WT, wt, IWT, iwt, IWTAE_128_Mask_2, IWTAE_512_Mask_2\n",
    "from utils.utils import zero_patches, zero_mask, set_seed, save_plot, create_inv_filters, create_filters, zero_pad\n",
    "\n",
    "# Set random seem for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "dataroot = '/hdd_e/han/data/celeba_512/'\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 4\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 512\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 128\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 300\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr_D = 0.0002\n",
    "lr_G = 0.0002\n",
    "lr_mn = 0.0001\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use an image folder dataset the way we have it setup.\n",
    "# Create the dataset\n",
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(512),\n",
    "                               transforms.CenterCrop(512),\n",
    "                               transforms.ToTensor(),\n",
    "#                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers, drop_last=True)\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Plot some training images\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# print(np.min(real_batch[0][0].numpy()))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:24], padding=2, normalize=False).cpu(),(1,2,0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wt(vimg, filters, levels=1):\n",
    "    bs = vimg.shape[0]\n",
    "    h = vimg.size(2)\n",
    "    w = vimg.size(3)\n",
    "    vimg = vimg.reshape(-1, 1, h, w)\n",
    "    padded = torch.nn.functional.pad(vimg,(2,2,2,2))\n",
    "    res = torch.nn.functional.conv2d(padded, Variable(filters[:,None]),stride=2)\n",
    "    if levels>1:\n",
    "        res[:,:1] = wt(res[:,:1], filters, levels-1)\n",
    "        res[:,:1,32:,:] = res[:,:1,32:,:]*1.\n",
    "        res[:,:1,:,32:] = res[:,:1,:,32:]*1.\n",
    "        res[:,1:] = res[:,1:]*1.\n",
    "    res = res.view(-1,2,h//2,w//2).transpose(1,2).contiguous().view(-1,1,h,w)\n",
    "    return res.reshape(bs, -1, h, w)\n",
    "\n",
    "def create_filters(device, wt_fn='bior2.2'):\n",
    "    w = pywt.Wavelet(wt_fn)\n",
    "    dec_hi = torch.Tensor(w.dec_hi[::-1]).to(device)\n",
    "    dec_lo = torch.Tensor(w.dec_lo[::-1]).to(device)\n",
    "    filters = torch.stack([dec_lo.unsqueeze(0)*dec_lo.unsqueeze(1),\n",
    "                           dec_lo.unsqueeze(0)*dec_hi.unsqueeze(1),\n",
    "                           dec_hi.unsqueeze(0)*dec_lo.unsqueeze(1),\n",
    "                           dec_hi.unsqueeze(0)*dec_hi.unsqueeze(1)], dim=0)\n",
    "    return filters\n",
    "\n",
    "def iwt(vres, inv_filters, levels=1):\n",
    "    bs = vres.shape[0]\n",
    "    h = vres.size(2)\n",
    "    w = vres.size(3)\n",
    "    vres = vres.reshape(-1, 1, h, w)\n",
    "    res = vres.contiguous().view(-1, h//2, 2, w//2).transpose(1, 2).contiguous().view(-1, 4, h//2, w//2).clone()\n",
    "    if levels > 1:\n",
    "        res[:,:1] = iwt(res[:,:1], inv_filters, levels=levels-1)\n",
    "    res = torch.nn.functional.conv_transpose2d(res, Variable(inv_filters[:,None]),stride=2)\n",
    "    res = res[:,:,2:-2,2:-2] #removing padding\n",
    "\n",
    "    return res.reshape(bs, -1, h, w)\n",
    "\n",
    "filters = create_filters(device='cuda:0')\n",
    "inv_filters = create_filters(device='cuda:0')\n",
    "\n",
    "cur_max = float('-inf')\n",
    "cur_min = float('inf')\n",
    "for i, data in enumerate(dataloader):\n",
    "    data_512 = data[0].to('cuda:0')\n",
    "    data_wt = wt(data_512, filters=filters, levels=4)[:, :, :32, :32]\n",
    "    cur_max = max(cur_max, torch.max(data_wt))\n",
    "    cur_min = min(cur_min, torch.min(data_wt))\n",
    "print(cur_max)\n",
    "print(cur_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = -1*cur_min\n",
    "scale = shift+cur_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "        \n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variational Autoencoder Code\n",
    "'''Sub class the nn module'''\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(AE, self).__init__() #not a python thing\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf//2, 4, 2, 1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(ndf//2, ndf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(ndf*8*4*4, nz)\n",
    "        self.fc2 = nn.Linear(ndf*8*4*4, nz)\n",
    "        self.fc3 = nn.Linear(nz, ndf*8*4*4)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( ndf*8*4*4, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf, ngf//2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf//2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d( ngf//2, nc, 4, 2, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        eps = torch.randn(*mu.size(),device=device)\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        z = mu + (std*eps)\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.fc3(z)\n",
    "        z = z.unsqueeze(-1).unsqueeze(-1)\n",
    "        x = self.decoder(z)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        x = self.decode(z)\n",
    "        return x, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Input: (N,Cin,Hin,Win)\n",
    "Output: (N,Cout,Hout,Wout) where Hout=(Hin−1)∗stride[0]−2∗padding[0]+kernel_size[0]+output_padding[0]  Wout=(Win−1)∗stride[1]−2∗padding[1]+kernel_size[1]+output_padding[1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide which device we want to run on\n",
    "device_2 = torch.device(\"cuda:1\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "# Create a copy of filters on the second device\n",
    "filters_d2 = create_filters(device=device_2)\n",
    "\n",
    "# Generator Code\n",
    "'''Sub class the nn module'''\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__() #not a python thing\n",
    "        self.ngpu = ngpu\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf//2, 4, 2, 1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(ndf//2, ndf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( ndf*8*4*4, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.InstanceNorm2d(ngf * 8,affine=True),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(ngf * 4,affine=True),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(ngf * 2,affine=True),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(ngf,affine=True),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf, ngf//2, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(ngf//2,affine=True),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d( ngf//2, nc, 4, 2, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "#         eps = torch.randn(b_size, 3, 64, 64, device=device)\n",
    "        enc = self.encoder(input)\n",
    "        enc = enc.unsqueeze(-1).unsqueeze(-1)\n",
    "        return self.decoder(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot():\n",
    "    with torch.no_grad():\n",
    "        netAE.eval()\n",
    "        data = next(iter(dataloader))\n",
    "        img = data[0][:32].to(device)\n",
    "        img = wt(img, filters=filters, levels=2)[:, :, :128, :128]\n",
    "        img = (img+shift)/scale\n",
    "        b_size = img.size(0)\n",
    "        grid,_,_ = netAE(img)\n",
    "        grid = grid.detach().cpu()\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"AE Reconstruction\")\n",
    "        plt.imshow(np.transpose(vutils.make_grid(grid, padding=2, normalize=False).cpu(),(1,2,0)))\n",
    "        \n",
    "        netG.eval()\n",
    "        grid = netG(grid.to(device_2)).detach().cpu()\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"GAN Reconstruction\")\n",
    "        plt.imshow(np.transpose(vutils.make_grid(grid, padding=2, normalize=False).cpu(),(1,2,0)))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE0 = 'cuda:0'\n",
    "DEVICE1 = 'cuda:1'\n",
    "\n",
    "checkpoint_dsvae_path = '/hdd_e/han/wtvae_results/models/dsvae_wt/DSVAE_WT.pt'\n",
    "# Load AE model for DSVAE\n",
    "netAE = AE(0).to(DEVICE0)\n",
    "checkpoint_dsvae = torch.load(checkpoint_dsvae_path, map_location=DEVICE0)\n",
    "netAE.load_state_dict(checkpoint_dsvae['netAE_state_dict'])\n",
    "netAE.eval()\n",
    "\n",
    "# Load DSVAE\n",
    "dsvae = Generator(0).to(DEVICE0)\n",
    "dsvae.load_state_dict(checkpoint_dsvae['netG_state_dict'])\n",
    "dsvae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(dataloader))\n",
    "img = data[0][:32].to(device)\n",
    "img = wt(img, filters=filters, levels=2)[:, :, :128, :128]\n",
    "img = (img+shift)/scale\n",
    "grid,_,_ = netAE(img)\n",
    "grid_dsvae = dsvae(grid).detach()\n",
    "plt.imshow(np.transpose(vutils.make_grid(grid_dsvae, padding=2, normalize=False).cpu(),(1,2,0)))\n",
    "plt.show()\n",
    "grid_dsvae_denorm = grid_dsvae * scale - shift\n",
    "plt.imshow(np.transpose(vutils.make_grid(grid_dsvae_denorm, padding=2, normalize=False).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved 128 and 512 mask networks\n",
    "# checkpoint_128_path = '/hdd_e/han/wtvae_results/models/iwtae128_noimgloss_lr1e-4_nonorm_2iwt_full/iwtvae_epoch300.pth'\n",
    "\n",
    "# iwt_model_128 = IWTAE_128_Mask_2(z_dim=100, num_iwt=2)\n",
    "# iwt_model_128.set_filters(inv_filters)\n",
    "# iwt_model_128.set_device(DEVICE1)\n",
    "# iwt_model_128 = iwt_model_128.to(DEVICE1)\n",
    "\n",
    "# checkpoint_128 = torch.load(checkpoint_128_path, map_location=DEVICE1)\n",
    "# iwt_model_128.load_state_dict(checkpoint_128['model_state_dict'])\n",
    "\n",
    "checkpoint_512_path = '/hdd_e/han/wtvae_results/models/iwtae512_overfit_noimgloss_lr1e-4_nonorm_full_rerun/iwtvae_epoch272.pth'\n",
    "iwt_model_512 = IWTAE_512_Mask_2(z_dim=100, num_iwt=3)\n",
    "iwt_model_512.set_device(DEVICE1)\n",
    "iwt_model_512 = iwt_model_512.to(DEVICE1)\n",
    "\n",
    "checkpoint_512 = torch.load(checkpoint_512_path, map_location=DEVICE1)\n",
    "iwt_model_512.load_state_dict(checkpoint_512['model_state_dict'])\n",
    "\n",
    "inv_filters = create_inv_filters(device=DEVICE0)\n",
    "iwt_fn = IWT(iwt=iwt, num_iwt=2)\n",
    "iwt_fn.set_filters(inv_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(data, shift, scale):\n",
    "    return data*scale - shift\n",
    "    \n",
    "def normalize(data, shift, scale):\n",
    "    return (data + shift) / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer_mn_128 = optim.Adam(iwt_model_128.parameters(), lr=lr_mn)\n",
    "optimizer_mn_512 = optim.Adam(iwt_model_512.parameters(), lr=lr_mn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iwt_model_128.train()\n",
    "\n",
    "# Finetuning 128 mask network\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        optimizer_mn_128.zero_grad()\n",
    "        \n",
    "        real_cpu = data[0].to(DEVICE0)\n",
    "        Y = wt(real_cpu, filters, levels=4)\n",
    "        Y_128 = wt(real_cpu, filters, levels=2)[:, :, :128, :128]\n",
    "        \n",
    "        # Get real mask at 128 x 128 (2 WT on that so each patch is 32 x 32)\n",
    "        real_mask = Y[:, :, :128, :128]\n",
    "        real_mask = zero_mask(real_mask, 2, 1)\n",
    "        real_mask = iwt(real_mask, inv_filters, levels=2)\n",
    "        \n",
    "        # Get reconstruction from AE & DSVAE\n",
    "        Y_128_norm = normalize(Y_128, shift, scale)\n",
    "        Y_128_norm_hat, _, _ = netAE(Y_128_norm)\n",
    "        Y_128_norm_hat = dsvae(Y_128_norm_hat)\n",
    "        Y_128_hat = denormalize(Y_128_norm_hat, shift, scale)\n",
    "        \n",
    "        # Apply WT twice to get 32 x 32 low freq patch and pad with zeros to create 128 x 128\n",
    "        Y_32 = wt(Y_128_hat, filters, levels=2)[:, :, :32, :32]\n",
    "        Y_32_padded = zero_pad(Y_32, 128, device=DEVICE0)\n",
    "        Y_32_low = iwt(Y_32_padded, inv_filters, levels=2)\n",
    "        \n",
    "        # Run through 128 mask network and backpropagate\n",
    "        recon_mask, mu, var = iwt_model_128(Y_32_low.to(DEVICE1))\n",
    "        loss, _, _ = iwt_model_128.loss_function(real_mask.to(DEVICE1), recon_mask, mu, var)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer_mn_128.step()\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            print('[{}/{}][{}/{}]\\t Loss: {}'.format(epoch, num_epochs, i, len(dataloader), loss.item()))\n",
    "    \n",
    "    \n",
    "    recon_mask_wt = wt(recon_mask.to(DEVICE0), filters, levels=2)\n",
    "    recon_img_wt = Y_32_padded + recon_mask_wt\n",
    "    recon_img = iwt(recon_img_wt, inv_filters, levels=2)\n",
    "    \n",
    "    save_image(real_mask.cpu(), './iwt_model_128_outputs/' + 'real_mask{}.png'.format(epoch))\n",
    "    save_image(recon_mask.cpu(), './iwt_model_128_outputs/' + 'recon_mask{}.png'.format(epoch))\n",
    "    save_image(recon_img.cpu(), './iwt_model_128_outputs/' + 'recon_img{}.png'.format(epoch))\n",
    "    save_image(Y_32_low.cpu(), './iwt_model_128_outputs/' + 'low_img{}.png'.format(epoch))\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        torch.save({\n",
    "            'model_state_dict': iwt_model_128.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_mn_128.state_dict(),\n",
    "            }, 'iwt_model_128_outputs/iwt_model_128_epoch{}.pth'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved 128 and 512 mask networks\n",
    "checkpoint_128_path = './iwt_model_128_outputs/iwt_model_128.pth'\n",
    "\n",
    "iwt_model_128 = IWTAE_128_Mask_2(z_dim=100, num_iwt=2)\n",
    "iwt_model_128.set_filters(inv_filters)\n",
    "iwt_model_128.set_device(DEVICE1)\n",
    "iwt_model_128 = iwt_model_128.to(DEVICE0)\n",
    "\n",
    "checkpoint_128 = torch.load(checkpoint_128_path, map_location=DEVICE0)\n",
    "iwt_model_128.load_state_dict(checkpoint_128['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iwt_model_128.eval()\n",
    "iwt_model_512.train()\n",
    "\n",
    "# Finetuning 128 mask network\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        optimizer_mn_512.zero_grad()\n",
    "        \n",
    "        real_cpu = data[0].to(DEVICE0)\n",
    "        Y = wt(real_cpu, filters, levels=4)\n",
    "        Y_128 = wt(real_cpu, filters, levels=2)[:, :, :128, :128]\n",
    "        \n",
    "        # Get real mask at 128 x 128 (2 WT on that so each patch is 32 x 32)\n",
    "        real_mask = zero_mask(Y, 2, 1)\n",
    "        real_mask = iwt(real_mask, inv_filters, levels=2)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get reconstruction from AE & DSVAE\n",
    "            Y_128_norm = normalize(Y_128, shift, scale)\n",
    "            Y_128_norm_hat, _, _ = netAE(Y_128_norm)\n",
    "            Y_128_norm_hat = dsvae(Y_128_norm_hat)\n",
    "            Y_128_hat = denormalize(Y_128_norm_hat, shift, scale)\n",
    "\n",
    "            # Apply WT twice to get 32 x 32 low freq patch and pad with zeros to create 128 x 128\n",
    "            Y_32 = wt(Y_128_hat, filters, levels=2)[:, :, :32, :32]\n",
    "            Y_32_padded = zero_pad(Y_32, 128, device=DEVICE0)\n",
    "            Y_32_low = iwt(Y_32_padded, inv_filters, levels=2)\n",
    "\n",
    "            # Run through 128 mask network and get reconstructed image\n",
    "            recon_mask_128, _, _ = iwt_model_128(Y_32_low)\n",
    "            recon_mask_128_wt = wt(recon_mask_128.to(DEVICE0), filters, levels=2)\n",
    "            recon_mask_128_wt[:, :, :32, :32] = Y_32\n",
    "            recon_img_128 = iwt(recon_mask_128_wt, inv_filters, levels=2)\n",
    "        \n",
    "            # Chain output of first mask network to next\n",
    "            recon_img_128_padded = zero_pad(recon_img_128, 512, device=DEVICE0)\n",
    "            recon_img_128_low = iwt(recon_img_128_padded, inv_filters, levels=2)\n",
    "        \n",
    "        recon_mask_512, mu, var = iwt_model_512(recon_img_128_low.to(DEVICE1))\n",
    "        loss, _, _ = iwt_model_512.loss_function(real_mask.to(DEVICE1), recon_mask_512, mu, var)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer_mn_512.step()\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print('[{}/{}][{}/{}]\\t Loss: {}'.format(epoch, num_epochs, i, len(dataloader), loss.item()))\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    \n",
    "    \n",
    "    recon_mask_512_wt = wt(recon_mask_512.to(DEVICE0), filters, levels=2)\n",
    "    recon_img_512_wt = recon_img_128_padded + recon_mask_512_wt\n",
    "    recon_img_512 = iwt(recon_img_512_wt, inv_filters, levels=2)\n",
    "    \n",
    "    save_image(real_mask.cpu(), './iwt_model_512_outputs/' + 'real_mask{}.png'.format(epoch))\n",
    "    save_image(recon_mask_512.cpu(), './iwt_model_512_outputs/' + 'recon_mask{}.png'.format(epoch))\n",
    "    save_image(recon_img_512.cpu(), './iwt_model_512_outputs/' + 'recon_img{}.png'.format(epoch))\n",
    "    save_image(recon_img_128_low.cpu(), './iwt_model_512_outputs/' + 'low_img{}.png'.format(epoch))\n",
    "    \n",
    "    if epoch % 30 == 0:\n",
    "        torch.save({\n",
    "            'model_state_dict': iwt_model_512.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_mn_512.state_dict(),\n",
    "            }, 'iwt_model_512_outputs/iwt_model_512_epoch{}.pth'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
